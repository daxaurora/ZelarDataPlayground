{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "piano-robin",
   "metadata": {},
   "source": [
    "This notebook creates a baseline model for predicting dementia from linguistic data.\n",
    "\n",
    "Data source: Ram Balasubramanium at Zelar Health.  \n",
    "Confirm: data originally from https://dementia.talkbank.org/access/English/Pitt.html\n",
    "\n",
    "Python library for parsing chat files:\n",
    "https://pylangacq.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import pylangacq\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-suffering",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Extract data from chat transcript files into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_control = 'PittData/PittTranscripts/Control/cookie/'\n",
    "files_dementia = 'PittData/PittTranscripts/Dementia/cookie/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count files in directories\n",
    "file_count_control = !ls $files_control | wc -l\n",
    "file_count_dementia = !ls $files_dementia| wc -l\n",
    "all_files_count = int(file_count_control[0]) + int(file_count_dementia[0])\n",
    "print('Control files:', file_count_control[0])\n",
    "print('Dementia files:', file_count_dementia[0])\n",
    "print('All files:', all_files_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into chat reader\n",
    "raw_data = pylangacq.Reader.from_dir(files_control)\n",
    "raw_data.append(pylangacq.Reader.from_dir(files_dementia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index from filenames and check index structure\n",
    "idx = [i['Media'].split(',')[0] for i in raw_data.headers()]\n",
    "print('Index length:', len(idx))\n",
    "print('Are all index values unique?', len(idx) == len(set(idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set column names for features and output variables\n",
    "cols = ['Group', 'MMSE', 'INV_Interjections', 'Repeats']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-helicopter",
   "metadata": {},
   "source": [
    "#### Extract output variables\n",
    "* _Control/Dementia_: Set binary variable for control to 0 and any dementia diagnosis to 1\n",
    "* _MMSE_: Not all files have a recorded MMSE value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine group values\n",
    "values = [i['Participants']['PAR']['group'] for i in raw_data.headers()]\n",
    "print('Unique group values:\\n', set(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of occurences of blank group value:\", values.count(''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-antique",
   "metadata": {},
   "source": [
    "Solution: If blank group value is assigned to Control, binary variable assignment matches original file designation.\n",
    "\n",
    "Optional further exploration: track down the file with the blank control value to confirm it was corrected assigned to the control group OR delete it from the raw dataset before extracting other values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set binary variable for Control=0 and Any Dementia Diagnosis=1\n",
    "group = [0 if i['Participants']['PAR']['group'] == 'Control' or i['Participants']['PAR']['group'] == ''\n",
    "         else 1\n",
    "         for i in raw_data.headers()]\n",
    "print('Dementia datapoints:', np.array(group).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMSE was coded by transcribers in the 'education' key in each transcript's header\n",
    "MMSE = [int(i['Participants']['PAR']['education']) if i['Participants']['PAR']['education'] != ''\n",
    "        else None\n",
    "        for i in raw_data.headers()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-platinum",
   "metadata": {},
   "source": [
    "#### Extract feature variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract number of interjections by investigator per interview\n",
    "INV = [len(i) for i in raw_data.utterances(participants=\"INV\", by_files=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract number of word or phrase repetitions\n",
    "repeat_word = '\\[/\\]'\n",
    "repeat_phrase = '\\[//\\]'\n",
    "\n",
    "reps = []\n",
    "for file in raw_data.utterances(by_files=True):\n",
    "    # Collect each file's utterances into a single string to search using regular expressions\n",
    "    utts_list = []\n",
    "    for utterance in file:\n",
    "        utts_list.append(utterance.tiers.get('PAR', ''))\n",
    "    utts = \"\".join(utts_list)\n",
    "    # add each file's number of repeated words and phrases to the variable\n",
    "    reps.append(len(re.findall(repeat_word, utts)) + \\\n",
    "                len(re.findall(repeat_phrase, utts)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into DataFrame\n",
    "data = pd.DataFrame(list(zip(group, \n",
    "                             MMSE,\n",
    "                             INV,\n",
    "                             reps\n",
    "                             )),\n",
    "                    columns=cols, \n",
    "                    index=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check combined index structure\n",
    "print('Index length:', data.index.size)\n",
    "print('Are all index values unique?', len(data.index) == len(set(data.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name index and dataframe\n",
    "data.index.name = 'Filename'\n",
    "data.name = 'Dementia Study - Cookie Theft Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-mystery",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows:\", data.shape[0], \"Columns:\", data.shape[1])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Index data type:', data.index.dtype)\n",
    "data.dtypes\n",
    "# Note: MMSE is interpreted by Python as type float because of missing MMSE data - NaN is type float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control group Summary statistics\n",
    "data[data.Group==0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dementia group Summary statistics\n",
    "data[data.Group==1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review missing values\n",
    "print('{0:<20} {1:^30}'.format('Samples Total', data.shape[0]))\n",
    "print()\n",
    "print('{0:<20} {1:^30}'.format('Variable', 'Samples with Missing Data'))\n",
    "print()\n",
    "# Data Index\n",
    "print('{0:<20} {1:^30}'.format(data.index.name, pd.isna(data.index).sum()))\n",
    "# Each column\n",
    "for i in range(len(cols)):\n",
    "    print('{0:<20} {1:^30}'.format(cols[i], pd.isna(data[cols[i]]).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-sauce",
   "metadata": {},
   "source": [
    "Are variables normally distributed?  **No**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,4)) \n",
    "for i in range(len(cols)):\n",
    "    ax = fig.add_subplot(1, 4, i+1)\n",
    "    ax.hist(data[cols[i]].dropna(), color='mediumvioletred') \n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.set_yticklabels('')\n",
    "    ax.set_xlabel(cols[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-tournament",
   "metadata": {},
   "source": [
    "## Baseline models\n",
    "For comparison, baseline models created from the dataset used for ADReSS challenge for the 2020 Interspeech Conference reached 75% accuracy and f1 scores using a larger subset of linguistic data from the same raw dataset.\\*\n",
    "\n",
    "The dataset used for the ADreSS challenge was an age and gender-matched subset of the full Pitt dataset, and included spontaenous speech. The model used 34 linguistic features extracted from the raw dataset, including duration, total utterances, MLU (mean length of utterance), type-token ratio, open-closed class word ratio, and percentages of 9 parts of speech. \n",
    "\n",
    "The baseline model created in this notebook uses the portion of the Pitt dataset in which participants are asked to describe the cookie theft picture commonly used in aphasia testing and uses only two features: number of interjections by interviewer and number of repeated words and phrases by the participant.\n",
    "\n",
    "\n",
    "\\* Luz S, Haider F, de la Fuente S, Fromm D, MacWhinney B. August 2020. *Alzheimerâ€™s Dementia Recognition through Spontaneous Speech: The ADReSS Challenge.* https://arxiv.org/abs/2004.06833v3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "data = data.sample(frac=1, random_state=8)\n",
    "print(\"Rows:\", data.shape[0], \"Columns:\", data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-attempt",
   "metadata": {},
   "source": [
    "### Model 1:  Predict Dementia vs. Control Group\n",
    "* Output variable is binary: Control vs. Dementia Group\n",
    "* Model types: Logistic Regression and Random Forest\n",
    "* Training to Test set ratio:  70%/30%\n",
    "* Variations of input variable: \n",
    "    - Single input feature:  Number of interjections by the Investigator.\n",
    "    - Single input feature:  Number of repeated words and phrases by participant\n",
    "    - Both features combined\n",
    "    \n",
    "\n",
    "Number of interviewer interjections is a potentially biased variable, if interviewer had knowledge of participant's dementia status or MMSE score before the interview.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-duration",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "Set up for cross-validation or bootstrapping for better model assessment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    \"\"\"Input: Input features (as data.SingleColumnName or data[['ColumnName', 'ColumnName']])\n",
    "              Single output variable as data.SingleColumnName\n",
    "              \n",
    "       Output: Training and test set with consisitent size, stratification and random_state\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y, \n",
    "                                                        test_size = 0.3,\n",
    "                                                        stratify = y, \n",
    "                                                        random_state = 32)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-scanning",
   "metadata": {},
   "source": [
    "First iteration of this model uses only one feature: number of interjections by interviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(data.INV_Interjections, data.Group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm equal control vs. dementia split in train vs. test sets\n",
    "print('Full group % dementia:', round(data['Group'].mean(), 4))\n",
    "print('Training set % dementia:', round(y_train.mean(), 4))\n",
    "print('Test set % dementia:', round(y_test.mean(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "logm = LogisticRegression()\n",
    "log_baseline = logm.fit(pd.DataFrame(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess fit of model\n",
    "print('Accuracy of baseline model is:')\n",
    "print(round(log_baseline.score(pd.DataFrame(X_test), y_test), 2))\n",
    "# AOC was similar to f1_score in most cases for this data\n",
    "#print('Area under the ROC curve is:')\n",
    "#print(round(roc_auc_score(y_test, log_baseline.predict_proba(pd.DataFrame(X_test))[:, 1]), 2))\n",
    "print('F1 score is:')\n",
    "print(round(f1_score(y_test, log_baseline.predict(pd.DataFrame(X_test))), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-exercise",
   "metadata": {},
   "source": [
    "Same model with different input feature: Number of repeated words and phrases per interview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainB, X_testB, y_trainB, y_testB = split_data(data.Repeats, data.Group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_baselineB = logm.fit(pd.DataFrame(X_trainB), y_trainB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess fit of model\n",
    "print('Accuracy of 2nd baseline model is:')\n",
    "print(round(log_baselineB.score(pd.DataFrame(X_testB), y_testB), 2))\n",
    "print('F1 score is:')\n",
    "print(round(f1_score(y_testB, log_baselineB.predict(pd.DataFrame(X_testB))), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-template",
   "metadata": {},
   "source": [
    "Same model using both features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainC, X_testC, y_trainC, y_testC = split_data(data[['Repeats', 'INV_Interjections']], data.Group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_baselineC = logm.fit(pd.DataFrame(X_trainC), y_trainC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess fit of model\n",
    "print('Accuracy of 3rd baseline model is:')\n",
    "print(round(log_baselineC.score(pd.DataFrame(X_testC), y_testC), 2))\n",
    "print('F1 score is:')\n",
    "print(round(f1_score(y_testC, log_baselineC.predict(pd.DataFrame(X_testC))), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-matrix",
   "metadata": {},
   "source": [
    "Try random forest model with same data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_baseline = rf.fit(pd.DataFrame(X_trainC), y_trainC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess fit of model\n",
    "print('Accuracy of random forest baseline model, without any hyperparameter tuning, is:')\n",
    "print(round(rf_baseline.score(pd.DataFrame(X_testC), y_testC), 2))\n",
    "print('F1 score is:')\n",
    "print(round(f1_score(y_testC, rf_baseline.predict(pd.DataFrame(X_testC))), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-heath",
   "metadata": {},
   "source": [
    "### Model 2: Prediction of MMSE\n",
    "\n",
    "Same parameters as Model 1 execpt:\n",
    "* Output variable: MMSE score ranges, binned according to density of MMSE value ranges in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first remove datapoints without MMSE scores\n",
    "data_MMSE = data[data.MMSE.notna()]\n",
    "print(\"Rows:\", data_MMSE.shape[0], \"Columns:\", data_MMSE.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin MMSE scores to use in a classification model\n",
    "bins = KBinsDiscretizer(n_bins=4, strategy=\"quantile\", encode=\"ordinal\")\n",
    "bins.fit(pd.DataFrame(data_MMSE.MMSE))\n",
    "data_MMSE['MMSE_bins'] = bins.transform(pd.DataFrame(data_MMSE.MMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_MMSE.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-campbell",
   "metadata": {},
   "source": [
    "Bin edges are set by default by KBinsDiscretizer based on frequency of values.  Bin edges set based on diagnostic criteria would be more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins.bin_edges_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = split_data(data_MMSE[['Repeats', 'INV_Interjections']], \n",
    "                                                  data_MMSE.MMSE_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_baselineMMSE = logm.fit(pd.DataFrame(X_train2), y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess fit of model\n",
    "print('Accuracy of baseline model predicting MMSE with logistic regression is:')\n",
    "print(round(log_baselineMMSE.score(pd.DataFrame(X_test2), y_test2), 2))\n",
    "print('F1 score is:')\n",
    "print(round(f1_score(y_test2, log_baselineMMSE.predict(pd.DataFrame(X_test2)), average='weighted'), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_baselineMMSE = rf.fit(pd.DataFrame(X_train2), y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of baseline model predicting MMSE with random forest classifier is:')\n",
    "print(round(rf_baselineMMSE.score(pd.DataFrame(X_test2), y_test2), 2))\n",
    "print('F1 score is:')\n",
    "print(round(f1_score(y_test2, rf_baselineMMSE.predict(pd.DataFrame(X_test2)), average='weighted'), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-shoot",
   "metadata": {},
   "source": [
    "The MMSE baseline model obviously isn't working well and needs work.  Next steps:  \n",
    "* track down f1 scores per bin.  Predictions may be better for some MMSE scores than others. \n",
    "* Could set up manual bins for MMSE based on diagnostic criteria.  For instance:  1-12, 13-20, 21-24, 25-27, 28-30.\n",
    "* MMSE prediction might also require more features, or some hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-african",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
